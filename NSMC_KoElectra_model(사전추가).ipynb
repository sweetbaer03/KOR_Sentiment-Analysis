{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "NSMC_KoElectra_model(사전추가).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sweetbaer03/KOR_Sentiment-Analysis/blob/main/NSMC_KoElectra_model(%EC%82%AC%EC%A0%84%EC%B6%94%EA%B0%80).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhS7-8FYk2P4"
      },
      "source": [
        "**NSMC Sentiment Analysis**\r\n",
        "\r\n",
        "KoElectra 모델 사용(감정사전추가)\r\n",
        "\r\n",
        "@misc{park2020koelectra,\r\n",
        "  author = {Park, Jangwon},\r\n",
        "  title = {KoELECTRA: Pretrained ELECTRA Model for Korean},\r\n",
        "  year = {2020},\r\n",
        "  publisher = {GitHub},\r\n",
        "  journal = {GitHub repository},\r\n",
        "  howpublished = {\\url{https://github.com/monologg/KoELECTRA}}\r\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZnlHwUzRjOD",
        "outputId": "3766caa5-3999-4efe-d429-0f6c5028dc83"
      },
      "source": [
        "!pip3 install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/0c/7d5950fcd80b029be0a8891727ba21e0cd27692c407c51261c3c921f6da3/transformers-4.1.1-py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 5.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Collecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 20.0MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 44.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=c3a1ceeb3f7a72c49ad1717053b089c5909378f984c5bc1fea00e5b15378f63b\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvbXBtaIRaZT"
      },
      "source": [
        "import tensorflow as tf\n",
        "import torch\n",
        "\n",
        "from transformers import ElectraTokenizer\n",
        "from transformers import ElectraForSequenceClassification, AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import datetime\n",
        "import urllib.request"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qcM30drlHoO"
      },
      "source": [
        "# 2. 데이터 로드"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74VXDJUmvfj2"
      },
      "source": [
        "github에서 필요한 데이터파일 다운"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1P3jXyTVve1F",
        "outputId": "e4f81fbc-772a-407e-c02b-ebede3b0d41f"
      },
      "source": [
        "!git clone https://github.com/sweetbaer03/KOR_Sentiment-Analysis.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'KOR_Sentiment-Analysis'...\n",
            "remote: Enumerating objects: 21, done.\u001b[K\n",
            "remote: Counting objects: 100% (21/21), done.\u001b[K\n",
            "remote: Compressing objects: 100% (19/19), done.\u001b[K\n",
            "remote: Total 21 (delta 5), reused 8 (delta 1), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (21/21), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPaFYshGlMNE"
      },
      "source": [
        "# 3. 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Brnz2yncvce-"
      },
      "source": [
        "MAX_LEN = 128\r\n",
        "batch_size = 32\r\n",
        "epochs = 5\r\n",
        "\r\n",
        "TOKEN_MODEL = 'monologg/koelectra-base-v3-discriminator'\r\n",
        "TRAIN_MODEL = 'monologg/koelectra-base-v3-discriminator'"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4u9LuZUp3vn"
      },
      "source": [
        "## 3.1 네이버리뷰 처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZRWUeBVRaZU"
      },
      "source": [
        "# train/test set 데이터 로드\n",
        "train_data = pd.read_csv(\"KOR_Sentiment-Analysis/data_in/ratings_train.txt\", sep='\\t')\n",
        "test_tmp = pd.read_csv(\"KOR_Sentiment-Analysis/data_in/ratings_test.txt\", sep='\\t')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rE-Veo2zlRqZ",
        "outputId": "959605b5-5917-4db1-c48d-e4c1c834add6"
      },
      "source": [
        "print(train_data.shape)\r\n",
        "print(test_tmp.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(150000, 3)\n",
            "(50000, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "-NgySiDaYHuo",
        "outputId": "4f41202e-2227-40a7-d686-9805d78cd6d0"
      },
      "source": [
        "train_data"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>document</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9976970</td>\n",
              "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3819312</td>\n",
              "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10265843</td>\n",
              "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9045019</td>\n",
              "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6483659</td>\n",
              "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149995</th>\n",
              "      <td>6222902</td>\n",
              "      <td>인간이 문제지.. 소는 뭔죄인가..</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149996</th>\n",
              "      <td>8549745</td>\n",
              "      <td>평점이 너무 낮아서...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149997</th>\n",
              "      <td>9311800</td>\n",
              "      <td>이게 뭐요? 한국인은 거들먹거리고 필리핀 혼혈은 착하다?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149998</th>\n",
              "      <td>2376369</td>\n",
              "      <td>청춘 영화의 최고봉.방황과 우울했던 날들의 자화상</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149999</th>\n",
              "      <td>9619869</td>\n",
              "      <td>한국 영화 최초로 수간하는 내용이 담긴 영화</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>150000 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              id                                           document  label\n",
              "0        9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
              "1        3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
              "2       10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
              "3        9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
              "4        6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1\n",
              "...          ...                                                ...    ...\n",
              "149995   6222902                                인간이 문제지.. 소는 뭔죄인가..      0\n",
              "149996   8549745                                      평점이 너무 낮아서...      1\n",
              "149997   9311800                    이게 뭐요? 한국인은 거들먹거리고 필리핀 혼혈은 착하다?      0\n",
              "149998   2376369                        청춘 영화의 최고봉.방황과 우울했던 날들의 자화상      1\n",
              "149999   9619869                           한국 영화 최초로 수간하는 내용이 담긴 영화      0\n",
              "\n",
              "[150000 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhhfMUiWnuag"
      },
      "source": [
        "## 3.2 캐글 데이터"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "wNYlwOFhRaZU",
        "outputId": "5f51e4e4-7c4e-4cbc-e0d5-03a77f01e8be"
      },
      "source": [
        "# 테스트(캐글) 데이터 로드\n",
        "test_csv = pd.read_csv('KOR_Sentiment-Analysis/data_in/ko_data.csv', encoding = 'cp949')\n",
        "test_csv.columns = ['id','document']\n",
        "test_csv.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>document</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>정말 많이 울었던 영화입니다.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>시간 낭비예요.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>포스터를 저렇게밖에 만들지 못했던 제작자의 소심함에 침을 뱉고 싶다.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>지금 봐도 재미있는 영화!!! 코믹과 감동!!! 그리고 요리!!!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>이걸 영화로 만드는 거야?얼마나 가는지 보자.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id                                document\n",
              "0   0                        정말 많이 울었던 영화입니다.\n",
              "1   1                                시간 낭비예요.\n",
              "2   2  포스터를 저렇게밖에 만들지 못했던 제작자의 소심함에 침을 뱉고 싶다.\n",
              "3   3    지금 봐도 재미있는 영화!!! 코믹과 감동!!! 그리고 요리!!!\n",
              "4   4               이걸 영화로 만드는 거야?얼마나 가는지 보자."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bvis9eEhqBnZ"
      },
      "source": [
        "# 감정사전 처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "hrvjnRjsqEWz",
        "outputId": "c8dee135-25ef-4366-ce14-35a61b613c0b"
      },
      "source": [
        "dic_data_value = pd.read_table('KOR_Sentiment-Analysis/data_in/SentiWord_Dict.txt', header=None)\r\n",
        "dic_data_value.rename(columns={dic_data_value.columns[0] : 'document', dic_data_value.columns[1] : 'label'}, inplace=True)\r\n",
        "dic_data_value.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>document</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>(-;</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>(;_;)</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>(^^)</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>(^-^)</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>(^^*</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  document  label\n",
              "0      (-;    1.0\n",
              "1    (;_;)   -1.0\n",
              "2     (^^)    1.0\n",
              "3    (^-^)    1.0\n",
              "4     (^^*    1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZzAhYyvqEZS",
        "outputId": "97e81d0b-c329-464d-bd04-7f5b0a993e6d"
      },
      "source": [
        "dic_data_value['document'] = dic_data_value['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 정규 표현식 수행\r\n",
        "dic_data_value['document'].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경\r\n",
        "dic_data_value = dic_data_value.dropna(how='any') # Null 값 제거\r\n",
        "dic_data_value = dic_data_value.dropna(how = 'any') # Null 값이 존재하는 행 제거\r\n",
        "print(dic_data_value.isnull().values.any()) # Null 값이 존재하는지 확인\r\n",
        "#감성이 없는 단어 삭제\r\n",
        "idx_ = dic_data_value[dic_data_value['label']==0.0].index\r\n",
        "dic_data_value = dic_data_value.drop(idx_)\r\n",
        "print(dic_data_value.groupby(['label']).size().reset_index(name = 'count'))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n",
            "   label  count\n",
            "0   -2.0   4799\n",
            "1   -1.0   5016\n",
            "2    1.0   2246\n",
            "3    2.0   2603\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKJyndztqEeW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "0c4ab5a0-19b0-4e52-e2e5-467b583a8ccf"
      },
      "source": [
        "# 감정을 숫자로 변환\r\n",
        "def emotion_labeling_dic(emotion):\r\n",
        "   return{-2.0 : 0, -1.0: 0, 1.0: 1, 2.0: 1}[emotion]\r\n",
        "\r\n",
        "emotion_labels = []\r\n",
        "\r\n",
        "for e in dic_data_value['label']:\r\n",
        "   emotion_labels.append(emotion_labeling_dic(e))\r\n",
        "\r\n",
        "dic_data_value['label'] = emotion_labels\r\n",
        "dic_data_value"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>document</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>ㅡㅡ</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>ㅅ</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>ㄱㅅ</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>ㄱㅇㄷ</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>가격이 싸다</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14849</th>\n",
              "      <td>오류</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14851</th>\n",
              "      <td>의혹</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14852</th>\n",
              "      <td>내팽개치다</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14853</th>\n",
              "      <td>횡령</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14854</th>\n",
              "      <td>불안증</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14664 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      document  label\n",
              "10          ㅡㅡ      0\n",
              "28           ㅅ      0\n",
              "41          ㄱㅅ      1\n",
              "42         ㄱㅇㄷ      1\n",
              "43      가격이 싸다      1\n",
              "...        ...    ...\n",
              "14849       오류      0\n",
              "14851       의혹      0\n",
              "14852    내팽개치다      0\n",
              "14853       횡령      0\n",
              "14854      불안증      0\n",
              "\n",
              "[14664 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "IvGX7tAMrwPd",
        "outputId": "57b60deb-0276-44c9-c6c1-4c79d2a55472"
      },
      "source": [
        "dic_data_value.reset_index(drop=True)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>document</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ㅡㅡ</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ㅅ</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ㄱㅅ</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ㄱㅇㄷ</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>가격이 싸다</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14659</th>\n",
              "      <td>오류</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14660</th>\n",
              "      <td>의혹</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14661</th>\n",
              "      <td>내팽개치다</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14662</th>\n",
              "      <td>횡령</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14663</th>\n",
              "      <td>불안증</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14664 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      document  label\n",
              "0           ㅡㅡ      0\n",
              "1            ㅅ      0\n",
              "2           ㄱㅅ      1\n",
              "3          ㄱㅇㄷ      1\n",
              "4       가격이 싸다      1\n",
              "...        ...    ...\n",
              "14659       오류      0\n",
              "14660       의혹      0\n",
              "14661    내팽개치다      0\n",
              "14662       횡령      0\n",
              "14663      불안증      0\n",
              "\n",
              "[14664 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNJtqxUlrwRZ",
        "outputId": "06029d54-feed-4917-bca2-1266f7ee63d6"
      },
      "source": [
        "print('전처리 후 사전 샘플의 개수 :',len(dic_data_value))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "전처리 후 사전 샘플의 개수 : 14664\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHGQwhY1YfUw"
      },
      "source": [
        "## 데이터 합치기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AnP2jpRYlOU"
      },
      "source": [
        "#합치기 위해 포멧 통일\r\n",
        "train_data.drop(labels='id', axis=\"columns\", inplace=True)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "rnOPqWprYlQu",
        "outputId": "dca65c63-f300-4d05-a673-8f51e69f855f"
      },
      "source": [
        "train = pd.concat([dic_data_value, train_data])\r\n",
        "#인덱스 재정렬\r\n",
        "train.reset_index(drop=True, inplace=True)\r\n",
        "train"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>document</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ㅡㅡ</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ㅅ</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ㄱㅅ</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ㄱㅇㄷ</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>가격이 싸다</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>164659</th>\n",
              "      <td>인간이 문제지.. 소는 뭔죄인가..</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>164660</th>\n",
              "      <td>평점이 너무 낮아서...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>164661</th>\n",
              "      <td>이게 뭐요? 한국인은 거들먹거리고 필리핀 혼혈은 착하다?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>164662</th>\n",
              "      <td>청춘 영화의 최고봉.방황과 우울했던 날들의 자화상</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>164663</th>\n",
              "      <td>한국 영화 최초로 수간하는 내용이 담긴 영화</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>164664 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                               document  label\n",
              "0                                    ㅡㅡ      0\n",
              "1                                     ㅅ      0\n",
              "2                                    ㄱㅅ      1\n",
              "3                                   ㄱㅇㄷ      1\n",
              "4                                가격이 싸다      1\n",
              "...                                 ...    ...\n",
              "164659              인간이 문제지.. 소는 뭔죄인가..      0\n",
              "164660                    평점이 너무 낮아서...      1\n",
              "164661  이게 뭐요? 한국인은 거들먹거리고 필리핀 혼혈은 착하다?      0\n",
              "164662      청춘 영화의 최고봉.방황과 우울했던 날들의 자화상      1\n",
              "164663         한국 영화 최초로 수간하는 내용이 담긴 영화      0\n",
              "\n",
              "[164664 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsBBf0oTWosE"
      },
      "source": [
        "## 데이터 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jih6DjJZRaZV"
      },
      "source": [
        "def getInputs(dataset):\n",
        "  data = dataset.copy(deep=True)\n",
        "\n",
        "  if 'document' in data.columns:\n",
        "    sentences = data['document']\n",
        "  else:\n",
        "    sentences = data['Sentence']\n",
        "\n",
        "  sentences = [\"[CLS] \" + str(sentence) + \" [SEP]\" for sentence in sentences]\n",
        "  \n",
        "  tokenizer = ElectraTokenizer.from_pretrained(TOKEN_MODEL, do_lower_case=False)\n",
        "  tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "\n",
        "  input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "  input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "  attention_masks = []\n",
        "  for seq in input_ids:\n",
        "      seq_mask = [float(i>0) for i in seq]\n",
        "      attention_masks.append(seq_mask)\n",
        "\n",
        "  return input_ids, attention_masks\n",
        "\n",
        "def getIndex(dataset):\n",
        "  data = dataset.copy(deep = True)\n",
        "  input_index = data.index.tolist()\n",
        "  return torch.tensor(input_index)\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbvzHvfQRaZV"
      },
      "source": [
        "labels1 = train['label'].values\n",
        "ratings_inputs1, ratings_masks1 = getInputs(train)\n",
        "\n",
        "ratings_test_labels = test_tmp['label'].values\n",
        "ratings_test_inputs, ratings_test_masks = getInputs(test_tmp)\n",
        "\n",
        "test_df_inputs, test_df_masks = getInputs(test_csv)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIzDZYOCbSOU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52a1a915-9e97-46de-9c6c-5e36fa64ee7e"
      },
      "source": [
        "print('전체 labels1 데이터의 개수: {}'.format(len(labels1)))\r\n",
        "print('전체 ratings_inputs1 데이터의 개수: {}'.format(len(ratings_inputs1)))\r\n",
        "print('전체 ratings_masks1 데이터의 개수: {}'.format(len(ratings_masks1)))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "전체 labels1 데이터의 개수: 164664\n",
            "전체 ratings_inputs1 데이터의 개수: 164664\n",
            "전체 ratings_masks1 데이터의 개수: 164664\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vX7I8mShTqp",
        "outputId": "1389805b-0c94-4888-93b7-a52ccf6fad10"
      },
      "source": [
        "print('전체 ratings_test_labels 데이터의 개수: {}'.format(len(ratings_test_labels)))\r\n",
        "print('전체 ratings_test_inputs 데이터의 개수: {}'.format(len(ratings_test_inputs)))\r\n",
        "print('전체 ratings_test_masks 데이터의 개수: {}'.format(len(ratings_test_masks)))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "전체 ratings_test_labels 데이터의 개수: 50000\n",
            "전체 ratings_test_inputs 데이터의 개수: 50000\n",
            "전체 ratings_test_masks 데이터의 개수: 50000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVU4aerOcdgL",
        "outputId": "9241c19a-0978-49b3-a31b-783e523c9994"
      },
      "source": [
        "print('전체 test_inputs 데이터의 개수: {}'.format(len(test_df_inputs)))\r\n",
        "print('전체 test_masks 데이터의 개수: {}'.format(len(test_df_masks)))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "전체 test_inputs 데이터의 개수: 11187\n",
            "전체 test_masks 데이터의 개수: 11187\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iQDiFDNYJNv"
      },
      "source": [
        "ratings_inputs = []\r\n",
        "ratings_inputs.extend(ratings_inputs1)\r\n",
        "#ratings_inputs.extend(ratings_inputs2)\r\n",
        "#ratings_inputs.extend(ratings_inputs3)\r\n",
        "labels = []\r\n",
        "labels.extend(labels1)\r\n",
        "#labels.extend(labels2)\r\n",
        "#labels.extend(labels3)\r\n",
        "ratings_masks = []\r\n",
        "ratings_masks.extend(ratings_masks1)\r\n",
        "#ratings_masks.extend(ratings_masks2)\r\n",
        "#ratings_masks.extend(ratings_masks3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxPIfdO3csv-",
        "outputId": "d368e403-048f-424d-d45f-9301dee7aa00"
      },
      "source": [
        "print('전체 labels 데이터의 개수: {}'.format(len(labels)))\r\n",
        "print('전체 ratings_inputs 데이터의 개수: {}'.format(len(ratings_inputs)))\r\n",
        "print('전체 ratings_masks 데이터의 개수: {}'.format(len(ratings_masks)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "전체 labels 데이터의 개수: 150000\n",
            "전체 ratings_inputs 데이터의 개수: 150000\n",
            "전체 ratings_masks 데이터의 개수: 150000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgd50Uw-WE1x",
        "outputId": "af95a6a6-c7fc-45f1-90c0-327a3e538e04"
      },
      "source": [
        "ratings_inputs1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   2,   44,  166, ...,    0,    0,    0],\n",
              "       [   2, 7987,    5, ...,    0,    0,    0],\n",
              "       [   2,    1,    3, ...,    0,    0,    0],\n",
              "       ...,\n",
              "       [   2, 5913, 2577, ...,    0,    0,    0],\n",
              "       [   2, 6652,  494, ...,    0,    0,    0],\n",
              "       [   2,  131,  494, ...,    0,    0,    0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQKaLnrY8x8M"
      },
      "source": [
        "마스크 씌우기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wj-HBiNRaZV"
      },
      "source": [
        "# 훈련셋과 검증셋으로 분리\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(ratings_inputs1, labels1, random_state=2018, test_size=0.1)\n",
        "\n",
        "# 어텐션 마스크를 훈련셋과 검증셋으로 분리\n",
        "train_masks, validation_masks, _, _ = train_test_split(ratings_masks1, ratings_inputs1, random_state=2018, test_size=0.1)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDm7OyTRRaZV"
      },
      "source": [
        "# 데이터를 파이토치의 텐서로 변환\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "validation_masks = torch.tensor(validation_masks)\t\t\t\n",
        "\n",
        "#네이버 테스트\n",
        "ratings_test_inputs = torch.tensor(ratings_test_inputs)\n",
        "ratings_test_labels = torch.tensor(ratings_test_labels)\n",
        "ratings_test_masks = torch.tensor(ratings_test_masks)\n",
        "\n",
        "#결과내야할 csv\n",
        "test_df_index = getIndex(test_csv)\n",
        "test_df_inputs = torch.tensor(test_df_inputs)\n",
        "test_df_masks = torch.tensor(test_df_masks)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQAkcRawRaZV"
      },
      "source": [
        "# 파이토치의 DataLoader로 입력, 마스크, 라벨을 묶어 데이터 설정\n",
        "# 학습시 배치 사이즈 만큼 데이터를 가져옴\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
        "\n",
        "test = TensorDataset(ratings_test_inputs, ratings_test_masks, ratings_test_labels)\n",
        "train_test_sampler = RandomSampler(test)\n",
        "train_test_dataloader = DataLoader(test, sampler=train_test_sampler, batch_size=batch_size)\n",
        "\n",
        "test_df_data = TensorDataset(test_df_index, test_df_inputs, test_df_masks)\n",
        "test_df_sampler = RandomSampler(test_df_data)\n",
        "test_df_dataloader = DataLoader(test_df_data, sampler=test_df_sampler, batch_size=batch_size)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmhWf3E3RaZV",
        "outputId": "ad819635-ce02-4935-d81b-58f2e52a8377"
      },
      "source": [
        "if torch.cuda.is_available():    \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print('No GPU available, using the CPU instead.')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wixPaJpERaZV",
        "outputId": "648bbd94-6152-4e39-de62-d6fdd80dda13"
      },
      "source": [
        "# ELECTRA 모델 생성\n",
        "model = ElectraForSequenceClassification.from_pretrained(TRAIN_MODEL, num_labels = 2)\n",
        "model.cuda()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at monologg/koelectra-base-v3-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
            "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ElectraForSequenceClassification(\n",
              "  (electra): ElectraModel(\n",
              "    (embeddings): ElectraEmbeddings(\n",
              "      (word_embeddings): Embedding(35000, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): ElectraEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): ElectraLayer(\n",
              "          (attention): ElectraAttention(\n",
              "            (self): ElectraSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): ElectraSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ElectraIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): ElectraOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): ElectraLayer(\n",
              "          (attention): ElectraAttention(\n",
              "            (self): ElectraSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): ElectraSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ElectraIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): ElectraOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): ElectraLayer(\n",
              "          (attention): ElectraAttention(\n",
              "            (self): ElectraSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): ElectraSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ElectraIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): ElectraOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): ElectraLayer(\n",
              "          (attention): ElectraAttention(\n",
              "            (self): ElectraSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): ElectraSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ElectraIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): ElectraOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): ElectraLayer(\n",
              "          (attention): ElectraAttention(\n",
              "            (self): ElectraSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): ElectraSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ElectraIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): ElectraOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): ElectraLayer(\n",
              "          (attention): ElectraAttention(\n",
              "            (self): ElectraSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): ElectraSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ElectraIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): ElectraOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): ElectraLayer(\n",
              "          (attention): ElectraAttention(\n",
              "            (self): ElectraSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): ElectraSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ElectraIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): ElectraOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): ElectraLayer(\n",
              "          (attention): ElectraAttention(\n",
              "            (self): ElectraSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): ElectraSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ElectraIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): ElectraOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): ElectraLayer(\n",
              "          (attention): ElectraAttention(\n",
              "            (self): ElectraSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): ElectraSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ElectraIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): ElectraOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): ElectraLayer(\n",
              "          (attention): ElectraAttention(\n",
              "            (self): ElectraSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): ElectraSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ElectraIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): ElectraOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): ElectraLayer(\n",
              "          (attention): ElectraAttention(\n",
              "            (self): ElectraSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): ElectraSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ElectraIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): ElectraOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): ElectraLayer(\n",
              "          (attention): ElectraAttention(\n",
              "            (self): ElectraSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): ElectraSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ElectraIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): ElectraOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (classifier): ElectraClassificationHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgg-nWx9RaZV"
      },
      "source": [
        "# 옵티마이저 설정\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 3e-5, # 학습률\n",
        "                  eps = 1e-8 # 0으로 나누는 것을 방지하기 위한 epsilon 값\n",
        "                )\n",
        "\n",
        "# 총 훈련 스텝 : 배치반복 횟수 * 에폭\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# 학습률을 조금씩 감소시키는 스케줄러 생성\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0,\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_abL_xT2RaZV"
      },
      "source": [
        "# 정확도 계산 함수\n",
        "def flat_accuracy(preds, labels):\n",
        "    \n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "# 시간 표시 함수\n",
        "def format_time(elapsed):\n",
        "\n",
        "    # 반올림\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # hh:mm:ss으로 형태 변경\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "\n",
        "# f1-score parameter\n",
        "from sklearn.metrics import f1_score\n",
        "f1_score_avg = []\n",
        "trues = []\n",
        "preds = []"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pvp0QdfJRaZV",
        "outputId": "7043506d-e98c-46e0-9da1-b280f43c09f1"
      },
      "source": [
        "# 재현을 위해 랜덤시드 고정\n",
        "seed_val = 42\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# 그래디언트 초기화\n",
        "model.zero_grad()\n",
        "\n",
        "# 에폭만큼 반복\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # 시작 시간 설정\n",
        "    t0 = time.time()\n",
        "  \n",
        "    # 로스 초기화\n",
        "    total_loss = 0\n",
        "\n",
        "    # 스코어 초기화\n",
        "    f1_score_avg = []\n",
        "    trues = []\n",
        "    preds = []\n",
        "\n",
        "    # 훈련모드로 변경\n",
        "    model.train()\n",
        "        \n",
        "    # 데이터로더에서 배치만큼 반복하여 가져옴\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        # 경과 정보 표시\n",
        "        if step % 500 == 0 and not step == 0:\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # 배치를 GPU에 넣음\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # 배치에서 데이터 추출\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        # Forward 수행                \n",
        "        outputs = model(b_input_ids, \n",
        "                        token_type_ids=None, \n",
        "                        attention_mask=b_input_mask, \n",
        "                        labels=b_labels)\n",
        "        \n",
        "        # 로스 구함\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # 총 로스 계산\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Backward 수행으로 그래디언트 계산\n",
        "        loss.backward()\n",
        "\n",
        "        # 그래디언트 클리핑\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # 그래디언트를 통해 가중치 파라미터 업데이트\n",
        "        optimizer.step()\n",
        "\n",
        "        # 스케줄러로 학습률 감소\n",
        "        scheduler.step()\n",
        "\n",
        "        # 그래디언트 초기화\n",
        "        model.zero_grad()\n",
        "\n",
        "    # 평균 로스 계산\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.5f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "\n",
        "    print(\"\")\n",
        "    \n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    #시작 시간 설정\n",
        "    t0 = time.time()\n",
        "\n",
        "    # 평가모드로 변경\n",
        "    model.eval()\n",
        "\n",
        "    # 변수 초기화\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # 데이터로더에서 배치만큼 반복하여 가져옴\n",
        "    for batch in validation_dataloader:\n",
        "        # 배치를 GPU에 넣음\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # 배치에서 데이터 추출\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # 그래디언트 계산 안함\n",
        "        with torch.no_grad():     \n",
        "            # Forward 수행\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        # 로스 구함\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # CPU로 데이터 이동\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        #f1\n",
        "        pred_flat = np.argmax(logits, axis=1).flatten()\n",
        "        trues_flat = label_ids.flatten()\n",
        "        trues.extend(trues_flat)\n",
        "        preds.extend(pred_flat)\n",
        "\n",
        "        # 출력 로직과 라벨을 비교하여 정확도 계산\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    print(\"  Accuracy: {0:.5f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  f1 score macro: {0: 2f}\".format(f1_score(y_true=trues,y_pred= preds, labels = [0,1], average='macro')))\n",
        "    print(\"  f1 score micro: {0: 2f}\".format(f1_score(y_true=trues, y_pred= preds, labels = [0,1],average='micro')))\n",
        "    print(\"  f1 score weighted: {0: 2f}\".format(f1_score(y_true=trues, y_pred= preds, labels = [0,1],average='weighted')))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 5 ========\n",
            "Training...\n",
            "  Batch   500  of  4,632.    Elapsed: 0:03:26.\n",
            "  Batch 1,000  of  4,632.    Elapsed: 0:06:51.\n",
            "  Batch 1,500  of  4,632.    Elapsed: 0:10:16.\n",
            "  Batch 2,000  of  4,632.    Elapsed: 0:13:42.\n",
            "  Batch 2,500  of  4,632.    Elapsed: 0:17:07.\n",
            "  Batch 3,000  of  4,632.    Elapsed: 0:20:33.\n",
            "  Batch 3,500  of  4,632.    Elapsed: 0:23:59.\n",
            "  Batch 4,000  of  4,632.    Elapsed: 0:27:25.\n",
            "  Batch 4,500  of  4,632.    Elapsed: 0:30:51.\n",
            "\n",
            "  Average training loss: 0.26711\n",
            "  Training epcoh took: 0:31:45\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.90985\n",
            "  f1 score macro:  0.909813\n",
            "  f1 score micro:  0.909820\n",
            "  f1 score weighted:  0.909833\n",
            "  Validation took: 0:01:03\n",
            "\n",
            "======== Epoch 2 / 5 ========\n",
            "Training...\n",
            "  Batch   500  of  4,632.    Elapsed: 0:03:26.\n",
            "  Batch 1,000  of  4,632.    Elapsed: 0:06:51.\n",
            "  Batch 1,500  of  4,632.    Elapsed: 0:10:16.\n",
            "  Batch 2,000  of  4,632.    Elapsed: 0:13:41.\n",
            "  Batch 2,500  of  4,632.    Elapsed: 0:17:06.\n",
            "  Batch 3,000  of  4,632.    Elapsed: 0:20:32.\n",
            "  Batch 3,500  of  4,632.    Elapsed: 0:23:59.\n",
            "  Batch 4,000  of  4,632.    Elapsed: 0:27:24.\n",
            "  Batch 4,500  of  4,632.    Elapsed: 0:30:50.\n",
            "\n",
            "  Average training loss: 0.17640\n",
            "  Training epcoh took: 0:31:44\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.91547\n",
            "  f1 score macro:  0.915363\n",
            "  f1 score micro:  0.915407\n",
            "  f1 score weighted:  0.915411\n",
            "  Validation took: 0:01:03\n",
            "\n",
            "======== Epoch 3 / 5 ========\n",
            "Training...\n",
            "  Batch   500  of  4,632.    Elapsed: 0:03:25.\n",
            "  Batch 1,000  of  4,632.    Elapsed: 0:06:49.\n",
            "  Batch 1,500  of  4,632.    Elapsed: 0:10:14.\n",
            "  Batch 2,000  of  4,632.    Elapsed: 0:13:38.\n",
            "  Batch 2,500  of  4,632.    Elapsed: 0:17:03.\n",
            "  Batch 3,000  of  4,632.    Elapsed: 0:20:27.\n",
            "  Batch 3,500  of  4,632.    Elapsed: 0:23:52.\n",
            "  Batch 4,000  of  4,632.    Elapsed: 0:27:16.\n",
            "  Batch 4,500  of  4,632.    Elapsed: 0:30:41.\n",
            "\n",
            "  Average training loss: 0.12243\n",
            "  Training epcoh took: 0:31:35\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.91426\n",
            "  f1 score macro:  0.914119\n",
            "  f1 score micro:  0.914192\n",
            "  f1 score weighted:  0.914182\n",
            "  Validation took: 0:01:03\n",
            "\n",
            "======== Epoch 4 / 5 ========\n",
            "Training...\n",
            "  Batch   500  of  4,632.    Elapsed: 0:03:24.\n",
            "  Batch 1,000  of  4,632.    Elapsed: 0:06:49.\n",
            "  Batch 1,500  of  4,632.    Elapsed: 0:10:14.\n",
            "  Batch 2,000  of  4,632.    Elapsed: 0:13:39.\n",
            "  Batch 2,500  of  4,632.    Elapsed: 0:17:03.\n",
            "  Batch 3,000  of  4,632.    Elapsed: 0:20:28.\n",
            "  Batch 3,500  of  4,632.    Elapsed: 0:23:52.\n",
            "  Batch 4,000  of  4,632.    Elapsed: 0:27:16.\n",
            "  Batch 4,500  of  4,632.    Elapsed: 0:30:40.\n",
            "\n",
            "  Average training loss: 0.08476\n",
            "  Training epcoh took: 0:31:33\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.91555\n",
            "  f1 score macro:  0.915499\n",
            "  f1 score micro:  0.915528\n",
            "  f1 score weighted:  0.915538\n",
            "  Validation took: 0:01:03\n",
            "\n",
            "======== Epoch 5 / 5 ========\n",
            "Training...\n",
            "  Batch   500  of  4,632.    Elapsed: 0:03:24.\n",
            "  Batch 1,000  of  4,632.    Elapsed: 0:06:48.\n",
            "  Batch 1,500  of  4,632.    Elapsed: 0:10:11.\n",
            "  Batch 2,000  of  4,632.    Elapsed: 0:13:35.\n",
            "  Batch 2,500  of  4,632.    Elapsed: 0:16:59.\n",
            "  Batch 3,000  of  4,632.    Elapsed: 0:20:23.\n",
            "  Batch 3,500  of  4,632.    Elapsed: 0:23:47.\n",
            "  Batch 4,000  of  4,632.    Elapsed: 0:27:11.\n",
            "  Batch 4,500  of  4,632.    Elapsed: 0:30:35.\n",
            "\n",
            "  Average training loss: 0.06192\n",
            "  Training epcoh took: 0:31:28\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.91640\n",
            "  f1 score macro:  0.916341\n",
            "  f1 score micro:  0.916378\n",
            "  f1 score weighted:  0.916385\n",
            "  Validation took: 0:01:03\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fwfAkC5RaZV"
      },
      "source": [
        "# 정확도 계산 함수\r\n",
        "def flat_accuracy(preds, labels):\r\n",
        "    \r\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\r\n",
        "    labels_flat = labels.flatten()\r\n",
        "\r\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\r\n",
        "\r\n",
        "# 시간 표시 함수\r\n",
        "def format_time(elapsed):\r\n",
        "\r\n",
        "    # 반올림\r\n",
        "    elapsed_rounded = int(round((elapsed)))\r\n",
        "    \r\n",
        "    # hh:mm:ss으로 형태 변경\r\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\r\n",
        "\r\n",
        "\r\n",
        "# f1-score parameter\r\n",
        "from sklearn.metrics import f1_score\r\n",
        "f1_score_avg = []\r\n",
        "trues = []\r\n",
        "preds = []"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Z6gnamYRaZV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dc7f086-75e5-43a6-dbee-7194a4e4a274"
      },
      "source": [
        "#시작 시간 설정\r\n",
        "t0 = time.time()\r\n",
        "\r\n",
        "# 평가모드로 변경\r\n",
        "model.eval()\r\n",
        "\r\n",
        "# 변수 초기화\r\n",
        "eval_loss, eval_accuracy = 0, 0\r\n",
        "nb_eval_steps, nb_eval_examples = 0, 0\r\n",
        "\r\n",
        "# 데이터로더에서 배치만큼 반복하여 가져옴\r\n",
        "for step, batch in enumerate(train_test_dataloader):\r\n",
        "    # 경과 정보 표시\r\n",
        "    if step % 100 == 0 and not step == 0:\r\n",
        "        elapsed = format_time(time.time() - t0)\r\n",
        "        print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_test_dataloader), elapsed))\r\n",
        "\r\n",
        "    # 배치를 GPU에 넣음\r\n",
        "    batch = tuple(t.to(device) for t in batch)\r\n",
        "    \r\n",
        "    # 배치에서 데이터 추출\r\n",
        "    b_input_ids, b_input_mask, b_labels = batch\r\n",
        "    \r\n",
        "    # 그래디언트 계산 안함\r\n",
        "    with torch.no_grad():     \r\n",
        "        # Forward 수행\r\n",
        "        outputs = model(b_input_ids, \r\n",
        "                        token_type_ids=None, \r\n",
        "                        attention_mask=b_input_mask)\r\n",
        "    \r\n",
        "    # 로스 구함\r\n",
        "    logits = outputs[0]\r\n",
        "\r\n",
        "    # CPU로 데이터 이동\r\n",
        "    logits = logits.detach().cpu().numpy()\r\n",
        "    label_ids = b_labels.to('cpu').numpy()\r\n",
        "\r\n",
        "   #\r\n",
        "    pred_flat = np.argmax(logits, axis=1).flatten()\r\n",
        "    trues_flat = label_ids.flatten()\r\n",
        "    trues.extend(trues_flat)\r\n",
        "    preds.extend(pred_flat)\r\n",
        "    \r\n",
        "    # 출력 로짓과 라벨을 비교하여 정확도 계산\r\n",
        "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\r\n",
        "    eval_accuracy += tmp_eval_accuracy\r\n",
        "    nb_eval_steps += 1\r\n",
        "\r\n",
        "print(\"\")\r\n",
        "print(\"Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\r\n",
        "print(\"  f1 score macro: {0: 2f}\".format(f1_score(y_true=trues,y_pred= preds, labels = [0,1], average='macro')))\r\n",
        "print(\"  f1 score micro: {0: 2f}\".format(f1_score(y_true=trues, y_pred= preds, labels = [0,1],average='micro')))\r\n",
        "print(\"  f1 score weighted: {0: 2f}\".format(f1_score(y_true=trues, y_pred= preds, labels = [0,1],average='weighted')))\r\n",
        "print(\"Test took: {:}\".format(format_time(time.time() - t0)))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Batch   100  of  1,563.    Elapsed: 0:00:12.\n",
            "  Batch   200  of  1,563.    Elapsed: 0:00:25.\n",
            "  Batch   300  of  1,563.    Elapsed: 0:00:37.\n",
            "  Batch   400  of  1,563.    Elapsed: 0:00:49.\n",
            "  Batch   500  of  1,563.    Elapsed: 0:01:01.\n",
            "  Batch   600  of  1,563.    Elapsed: 0:01:14.\n",
            "  Batch   700  of  1,563.    Elapsed: 0:01:26.\n",
            "  Batch   800  of  1,563.    Elapsed: 0:01:38.\n",
            "  Batch   900  of  1,563.    Elapsed: 0:01:51.\n",
            "  Batch 1,000  of  1,563.    Elapsed: 0:02:03.\n",
            "  Batch 1,100  of  1,563.    Elapsed: 0:02:15.\n",
            "  Batch 1,200  of  1,563.    Elapsed: 0:02:27.\n",
            "  Batch 1,300  of  1,563.    Elapsed: 0:02:40.\n",
            "  Batch 1,400  of  1,563.    Elapsed: 0:02:52.\n",
            "  Batch 1,500  of  1,563.    Elapsed: 0:03:04.\n",
            "\n",
            "Accuracy: 0.91\n",
            "  f1 score macro:  0.908987\n",
            "  f1 score micro:  0.909000\n",
            "  f1 score weighted:  0.908995\n",
            "Test took: 0:03:12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFPM1iimhJrI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UL5PGNTeqZP1"
      },
      "source": [
        "tmp_test_dataloader = DataLoader(test_df_data, sampler=test_df_sampler, batch_size=1)\r\n",
        "test_result = test_csv.copy(deep = True)\r\n",
        "test_result['Predicted'] = 'default'\r\n",
        "classes = [0, 1]"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EH8VX6TeqacD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "c1a25fbd-5066-4d01-a334-fa62e2437b46"
      },
      "source": [
        "test_result"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>document</th>\n",
              "      <th>Predicted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>정말 많이 울었던 영화입니다.</td>\n",
              "      <td>default</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>시간 낭비예요.</td>\n",
              "      <td>default</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>포스터를 저렇게밖에 만들지 못했던 제작자의 소심함에 침을 뱉고 싶다.</td>\n",
              "      <td>default</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>지금 봐도 재미있는 영화!!! 코믹과 감동!!! 그리고 요리!!!</td>\n",
              "      <td>default</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>이걸 영화로 만드는 거야?얼마나 가는지 보자.</td>\n",
              "      <td>default</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11182</th>\n",
              "      <td>11182</td>\n",
              "      <td>이 영화를 커플에게 추천합니다. 영화관에 가다보면 평생 잊지 못할 추억이 하나 생길...</td>\n",
              "      <td>default</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11183</th>\n",
              "      <td>11183</td>\n",
              "      <td>심심__ 그냥 한효주 cf</td>\n",
              "      <td>default</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11184</th>\n",
              "      <td>11184</td>\n",
              "      <td>공감해서 눈물나는 영화. 안 보신분들이 전부 제가 울었다고 하면 의아해하실텐데 보면...</td>\n",
              "      <td>default</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11185</th>\n",
              "      <td>11185</td>\n",
              "      <td>오토바이 신은 최고네요.</td>\n",
              "      <td>default</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11186</th>\n",
              "      <td>11186</td>\n",
              "      <td>개병헌 쓰면 엉망이 된다ㅋㅋㅋ</td>\n",
              "      <td>default</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11187 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          id                                           document Predicted\n",
              "0          0                                   정말 많이 울었던 영화입니다.   default\n",
              "1          1                                           시간 낭비예요.   default\n",
              "2          2             포스터를 저렇게밖에 만들지 못했던 제작자의 소심함에 침을 뱉고 싶다.   default\n",
              "3          3               지금 봐도 재미있는 영화!!! 코믹과 감동!!! 그리고 요리!!!   default\n",
              "4          4                          이걸 영화로 만드는 거야?얼마나 가는지 보자.   default\n",
              "...      ...                                                ...       ...\n",
              "11182  11182  이 영화를 커플에게 추천합니다. 영화관에 가다보면 평생 잊지 못할 추억이 하나 생길...   default\n",
              "11183  11183                                     심심__ 그냥 한효주 cf   default\n",
              "11184  11184  공감해서 눈물나는 영화. 안 보신분들이 전부 제가 울었다고 하면 의아해하실텐데 보면...   default\n",
              "11185  11185                                      오토바이 신은 최고네요.   default\n",
              "11186  11186                                   개병헌 쓰면 엉망이 된다ㅋㅋㅋ   default\n",
              "\n",
              "[11187 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msu3YjraRaZV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "467b9877-d359-41b8-ba2c-bcd2603abf72"
      },
      "source": [
        "#시작 시간 설정\n",
        "t0 = time.time()\n",
        "\n",
        "# 평가모드로 변경\n",
        "model.eval()\n",
        "\n",
        "# 변수 초기화\n",
        "nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "# 데이터로더에서 배치만큼 반복하여 가져옴\n",
        "for step, batch in enumerate(tmp_test_dataloader):\n",
        "    # 경과 정보 표시\n",
        "    if step % 100 == 0 and not step == 0:\n",
        "        elapsed = format_time(time.time() - t0)\n",
        "        print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(test_df_dataloader), elapsed))\n",
        "\n",
        "    # 배치를 GPU에 넣음\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    \n",
        "    # 배치에서 데이터 추출\n",
        "    b_index, b_input_ids, b_input_mask = batch\n",
        "    \n",
        "    # 그래디언트 계산 안함\n",
        "    with torch.no_grad():     \n",
        "        # Forward 수행\n",
        "        outputs = model(b_input_ids, \n",
        "                        token_type_ids=None, \n",
        "                        attention_mask=b_input_mask)\n",
        "    \n",
        "    # 로스 구함\n",
        "    logits = outputs[0]\n",
        "\n",
        "    # CPU로 데이터 이동\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    idx = b_index.item()\n",
        "    test_result['Predicted'][idx] = classes[np.argmax(logits)]\n",
        "    \n",
        "\n",
        "    nb_eval_steps += 1\n",
        "\n",
        "print(\"\")\n",
        "print(\"Test took: {:}\".format(format_time(time.time() - t0)))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:36: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  Batch   100  of    350.    Elapsed: 0:00:02.\n",
            "  Batch   200  of    350.    Elapsed: 0:00:03.\n",
            "  Batch   300  of    350.    Elapsed: 0:00:05.\n",
            "  Batch   400  of    350.    Elapsed: 0:00:06.\n",
            "  Batch   500  of    350.    Elapsed: 0:00:08.\n",
            "  Batch   600  of    350.    Elapsed: 0:00:09.\n",
            "  Batch   700  of    350.    Elapsed: 0:00:11.\n",
            "  Batch   800  of    350.    Elapsed: 0:00:13.\n",
            "  Batch   900  of    350.    Elapsed: 0:00:14.\n",
            "  Batch 1,000  of    350.    Elapsed: 0:00:16.\n",
            "  Batch 1,100  of    350.    Elapsed: 0:00:17.\n",
            "  Batch 1,200  of    350.    Elapsed: 0:00:19.\n",
            "  Batch 1,300  of    350.    Elapsed: 0:00:20.\n",
            "  Batch 1,400  of    350.    Elapsed: 0:00:22.\n",
            "  Batch 1,500  of    350.    Elapsed: 0:00:23.\n",
            "  Batch 1,600  of    350.    Elapsed: 0:00:25.\n",
            "  Batch 1,700  of    350.    Elapsed: 0:00:26.\n",
            "  Batch 1,800  of    350.    Elapsed: 0:00:28.\n",
            "  Batch 1,900  of    350.    Elapsed: 0:00:29.\n",
            "  Batch 2,000  of    350.    Elapsed: 0:00:31.\n",
            "  Batch 2,100  of    350.    Elapsed: 0:00:33.\n",
            "  Batch 2,200  of    350.    Elapsed: 0:00:34.\n",
            "  Batch 2,300  of    350.    Elapsed: 0:00:36.\n",
            "  Batch 2,400  of    350.    Elapsed: 0:00:37.\n",
            "  Batch 2,500  of    350.    Elapsed: 0:00:39.\n",
            "  Batch 2,600  of    350.    Elapsed: 0:00:40.\n",
            "  Batch 2,700  of    350.    Elapsed: 0:00:42.\n",
            "  Batch 2,800  of    350.    Elapsed: 0:00:44.\n",
            "  Batch 2,900  of    350.    Elapsed: 0:00:45.\n",
            "  Batch 3,000  of    350.    Elapsed: 0:00:47.\n",
            "  Batch 3,100  of    350.    Elapsed: 0:00:48.\n",
            "  Batch 3,200  of    350.    Elapsed: 0:00:50.\n",
            "  Batch 3,300  of    350.    Elapsed: 0:00:51.\n",
            "  Batch 3,400  of    350.    Elapsed: 0:00:53.\n",
            "  Batch 3,500  of    350.    Elapsed: 0:00:55.\n",
            "  Batch 3,600  of    350.    Elapsed: 0:00:56.\n",
            "  Batch 3,700  of    350.    Elapsed: 0:00:58.\n",
            "  Batch 3,800  of    350.    Elapsed: 0:00:59.\n",
            "  Batch 3,900  of    350.    Elapsed: 0:01:01.\n",
            "  Batch 4,000  of    350.    Elapsed: 0:01:02.\n",
            "  Batch 4,100  of    350.    Elapsed: 0:01:04.\n",
            "  Batch 4,200  of    350.    Elapsed: 0:01:06.\n",
            "  Batch 4,300  of    350.    Elapsed: 0:01:07.\n",
            "  Batch 4,400  of    350.    Elapsed: 0:01:09.\n",
            "  Batch 4,500  of    350.    Elapsed: 0:01:10.\n",
            "  Batch 4,600  of    350.    Elapsed: 0:01:12.\n",
            "  Batch 4,700  of    350.    Elapsed: 0:01:13.\n",
            "  Batch 4,800  of    350.    Elapsed: 0:01:15.\n",
            "  Batch 4,900  of    350.    Elapsed: 0:01:16.\n",
            "  Batch 5,000  of    350.    Elapsed: 0:01:18.\n",
            "  Batch 5,100  of    350.    Elapsed: 0:01:19.\n",
            "  Batch 5,200  of    350.    Elapsed: 0:01:21.\n",
            "  Batch 5,300  of    350.    Elapsed: 0:01:23.\n",
            "  Batch 5,400  of    350.    Elapsed: 0:01:24.\n",
            "  Batch 5,500  of    350.    Elapsed: 0:01:26.\n",
            "  Batch 5,600  of    350.    Elapsed: 0:01:27.\n",
            "  Batch 5,700  of    350.    Elapsed: 0:01:29.\n",
            "  Batch 5,800  of    350.    Elapsed: 0:01:30.\n",
            "  Batch 5,900  of    350.    Elapsed: 0:01:32.\n",
            "  Batch 6,000  of    350.    Elapsed: 0:01:33.\n",
            "  Batch 6,100  of    350.    Elapsed: 0:01:35.\n",
            "  Batch 6,200  of    350.    Elapsed: 0:01:37.\n",
            "  Batch 6,300  of    350.    Elapsed: 0:01:38.\n",
            "  Batch 6,400  of    350.    Elapsed: 0:01:40.\n",
            "  Batch 6,500  of    350.    Elapsed: 0:01:41.\n",
            "  Batch 6,600  of    350.    Elapsed: 0:01:43.\n",
            "  Batch 6,700  of    350.    Elapsed: 0:01:44.\n",
            "  Batch 6,800  of    350.    Elapsed: 0:01:46.\n",
            "  Batch 6,900  of    350.    Elapsed: 0:01:47.\n",
            "  Batch 7,000  of    350.    Elapsed: 0:01:49.\n",
            "  Batch 7,100  of    350.    Elapsed: 0:01:50.\n",
            "  Batch 7,200  of    350.    Elapsed: 0:01:52.\n",
            "  Batch 7,300  of    350.    Elapsed: 0:01:54.\n",
            "  Batch 7,400  of    350.    Elapsed: 0:01:55.\n",
            "  Batch 7,500  of    350.    Elapsed: 0:01:57.\n",
            "  Batch 7,600  of    350.    Elapsed: 0:01:58.\n",
            "  Batch 7,700  of    350.    Elapsed: 0:02:00.\n",
            "  Batch 7,800  of    350.    Elapsed: 0:02:01.\n",
            "  Batch 7,900  of    350.    Elapsed: 0:02:03.\n",
            "  Batch 8,000  of    350.    Elapsed: 0:02:04.\n",
            "  Batch 8,100  of    350.    Elapsed: 0:02:06.\n",
            "  Batch 8,200  of    350.    Elapsed: 0:02:07.\n",
            "  Batch 8,300  of    350.    Elapsed: 0:02:09.\n",
            "  Batch 8,400  of    350.    Elapsed: 0:02:10.\n",
            "  Batch 8,500  of    350.    Elapsed: 0:02:12.\n",
            "  Batch 8,600  of    350.    Elapsed: 0:02:14.\n",
            "  Batch 8,700  of    350.    Elapsed: 0:02:15.\n",
            "  Batch 8,800  of    350.    Elapsed: 0:02:17.\n",
            "  Batch 8,900  of    350.    Elapsed: 0:02:18.\n",
            "  Batch 9,000  of    350.    Elapsed: 0:02:20.\n",
            "  Batch 9,100  of    350.    Elapsed: 0:02:21.\n",
            "  Batch 9,200  of    350.    Elapsed: 0:02:23.\n",
            "  Batch 9,300  of    350.    Elapsed: 0:02:24.\n",
            "  Batch 9,400  of    350.    Elapsed: 0:02:26.\n",
            "  Batch 9,500  of    350.    Elapsed: 0:02:27.\n",
            "  Batch 9,600  of    350.    Elapsed: 0:02:29.\n",
            "  Batch 9,700  of    350.    Elapsed: 0:02:31.\n",
            "  Batch 9,800  of    350.    Elapsed: 0:02:32.\n",
            "  Batch 9,900  of    350.    Elapsed: 0:02:34.\n",
            "  Batch 10,000  of    350.    Elapsed: 0:02:35.\n",
            "  Batch 10,100  of    350.    Elapsed: 0:02:37.\n",
            "  Batch 10,200  of    350.    Elapsed: 0:02:38.\n",
            "  Batch 10,300  of    350.    Elapsed: 0:02:40.\n",
            "  Batch 10,400  of    350.    Elapsed: 0:02:41.\n",
            "  Batch 10,500  of    350.    Elapsed: 0:02:43.\n",
            "  Batch 10,600  of    350.    Elapsed: 0:02:45.\n",
            "  Batch 10,700  of    350.    Elapsed: 0:02:46.\n",
            "  Batch 10,800  of    350.    Elapsed: 0:02:48.\n",
            "  Batch 10,900  of    350.    Elapsed: 0:02:49.\n",
            "  Batch 11,000  of    350.    Elapsed: 0:02:51.\n",
            "  Batch 11,100  of    350.    Elapsed: 0:02:52.\n",
            "\n",
            "Test took: 0:02:54\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfPhzGBrpCM2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "afe2f08e-a8e4-4dbb-9ed5-d2ced1bc4c20"
      },
      "source": [
        "test_result"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>document</th>\n",
              "      <th>Predicted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>정말 많이 울었던 영화입니다.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>시간 낭비예요.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>포스터를 저렇게밖에 만들지 못했던 제작자의 소심함에 침을 뱉고 싶다.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>지금 봐도 재미있는 영화!!! 코믹과 감동!!! 그리고 요리!!!</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>이걸 영화로 만드는 거야?얼마나 가는지 보자.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11182</th>\n",
              "      <td>11182</td>\n",
              "      <td>이 영화를 커플에게 추천합니다. 영화관에 가다보면 평생 잊지 못할 추억이 하나 생길...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11183</th>\n",
              "      <td>11183</td>\n",
              "      <td>심심__ 그냥 한효주 cf</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11184</th>\n",
              "      <td>11184</td>\n",
              "      <td>공감해서 눈물나는 영화. 안 보신분들이 전부 제가 울었다고 하면 의아해하실텐데 보면...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11185</th>\n",
              "      <td>11185</td>\n",
              "      <td>오토바이 신은 최고네요.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11186</th>\n",
              "      <td>11186</td>\n",
              "      <td>개병헌 쓰면 엉망이 된다ㅋㅋㅋ</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11187 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          id                                           document Predicted\n",
              "0          0                                   정말 많이 울었던 영화입니다.         1\n",
              "1          1                                           시간 낭비예요.         0\n",
              "2          2             포스터를 저렇게밖에 만들지 못했던 제작자의 소심함에 침을 뱉고 싶다.         1\n",
              "3          3               지금 봐도 재미있는 영화!!! 코믹과 감동!!! 그리고 요리!!!         1\n",
              "4          4                          이걸 영화로 만드는 거야?얼마나 가는지 보자.         0\n",
              "...      ...                                                ...       ...\n",
              "11182  11182  이 영화를 커플에게 추천합니다. 영화관에 가다보면 평생 잊지 못할 추억이 하나 생길...         1\n",
              "11183  11183                                     심심__ 그냥 한효주 cf         0\n",
              "11184  11184  공감해서 눈물나는 영화. 안 보신분들이 전부 제가 울었다고 하면 의아해하실텐데 보면...         1\n",
              "11185  11185                                      오토바이 신은 최고네요.         0\n",
              "11186  11186                                   개병헌 쓰면 엉망이 된다ㅋㅋㅋ         0\n",
              "\n",
              "[11187 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqapfnjRogrS"
      },
      "source": [
        "test_result.drop(labels='document', axis=\"columns\", inplace=True)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRcq00kRRaZV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "5cb2959d-4dd4-4b45-c955-3120db2680d0"
      },
      "source": [
        "DATA_OUT_NAME = \"NSMC_KoElectra_v3_add_epo3_MaxLen:\"+ str(MAX_LEN) + \"_BchSize:\"+ str(batch_size) + \".csv\"\r\n",
        "\r\n",
        "test_csv = test_result.to_csv(DATA_OUT_NAME, columns=['id', 'Predicted'], index=False, encoding = 'cp949')\r\n",
        "from google.colab import files\r\n",
        "\r\n",
        "files.download(DATA_OUT_NAME)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_bf2b0aae-4c46-4008-beb5-861015fedef3\", \"NSMC_KoElectra_v3_add_epo3_MaxLen:128_BchSize:32.csv\", 78399)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oI1J0GZ4RaZV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}